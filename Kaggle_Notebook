{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Competition Notebook:\"LMSYS - Chatbot Arena Human Preference Predictions\"","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I am conducting data exploration, analysis, and modeling in order to generate predictions for which Chatbot model will be preferred for a given prompt.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T18:43:44.881885Z","iopub.execute_input":"2024-05-17T18:43:44.882456Z","iopub.status.idle":"2024-05-17T18:43:45.364875Z","shell.execute_reply.started":"2024-05-17T18:43:44.882405Z","shell.execute_reply":"2024-05-17T18:43:45.363360Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\n/kaggle/input/lmsys-chatbot-arena/train.csv\n/kaggle/input/lmsys-chatbot-arena/test.csv\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport seaborn as sns\nsns.set(font_scale=2)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.tokenize import RegexpTokenizer, word_tokenize\nfrom sklearn.metrics import log_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom nltk import FreqDist\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\n\nfrom gensim.models import word2vec","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:43:45.659160Z","iopub.execute_input":"2024-05-17T18:43:45.659720Z","iopub.status.idle":"2024-05-17T18:44:01.110036Z","shell.execute_reply.started":"2024-05-17T18:43:45.659684Z","shell.execute_reply":"2024-05-17T18:44:01.108872Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras==2.15.0","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:01.112114Z","iopub.execute_input":"2024-05-17T18:44:01.112756Z","iopub.status.idle":"2024-05-17T18:44:16.731067Z","shell.execute_reply.started":"2024-05-17T18:44:01.112720Z","shell.execute_reply":"2024-05-17T18:44:16.728961Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras==2.15.0 in /opt/conda/lib/python3.10/site-packages (2.15.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tensorflow import keras\nimport keras\nimport keras_nlp\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\n\nfrom tqdm.auto import tqdm\n\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:16.733605Z","iopub.execute_input":"2024-05-17T18:44:16.734285Z","iopub.status.idle":"2024-05-17T18:44:21.884831Z","shell.execute_reply.started":"2024-05-17T18:44:16.734190Z","shell.execute_reply":"2024-05-17T18:44:21.881433Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-17 18:44:17.157434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 18:44:17.157521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 18:44:17.161083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from tensorflow import keras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/layers/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malibi_bias\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlibiBias\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcached_multi_head_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CachedMultiHeadAttention\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mf_net_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FNetEncoder\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/layers/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The KerasNLP Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcached_multi_head_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CachedMultiHeadAttention,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mf_net_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FNetEncoder\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked_lm_head\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedLMHead\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/cached_multi_head_attention.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The KerasNLP Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_nlp_export\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/api_export.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The KerasNLP Authors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnamex\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/backend/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/backend/ops.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403, F401\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403, F401\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_core'"],"ename":"ModuleNotFoundError","evalue":"No module named 'keras_core'","output_type":"error"}]},{"cell_type":"markdown","source":"Loading the train and test datasets:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:39.319666Z","iopub.execute_input":"2024-05-17T18:44:39.320193Z","iopub.status.idle":"2024-05-17T18:44:41.713390Z","shell.execute_reply.started":"2024-05-17T18:44:39.320153Z","shell.execute_reply":"2024-05-17T18:44:41.711742Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.715642Z","iopub.execute_input":"2024-05-17T18:44:41.716071Z","iopub.status.idle":"2024-05-17T18:44:41.740800Z","shell.execute_reply.started":"2024-05-17T18:44:41.716035Z","shell.execute_reply":"2024-05-17T18:44:41.739390Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.742250Z","iopub.execute_input":"2024-05-17T18:44:41.742651Z","iopub.status.idle":"2024-05-17T18:44:41.757797Z","shell.execute_reply.started":"2024-05-17T18:44:41.742618Z","shell.execute_reply":"2024-05-17T18:44:41.755969Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.761005Z","iopub.execute_input":"2024-05-17T18:44:41.761653Z","iopub.status.idle":"2024-05-17T18:44:41.820185Z","shell.execute_reply.started":"2024-05-17T18:44:41.761614Z","shell.execute_reply":"2024-05-17T18:44:41.818757Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.821833Z","iopub.execute_input":"2024-05-17T18:44:41.822258Z","iopub.status.idle":"2024-05-17T18:44:41.854066Z","shell.execute_reply.started":"2024-05-17T18:44:41.822225Z","shell.execute_reply":"2024-05-17T18:44:41.852755Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                 id  winner_model_a  winner_model_b    winner_tie\ncount  5.747700e+04    57477.000000    57477.000000  57477.000000\nmean   2.142564e+09        0.349079        0.341911      0.309011\nstd    1.238327e+09        0.476683        0.474354      0.462090\nmin    3.019200e+04        0.000000        0.000000      0.000000\n25%    1.071821e+09        0.000000        0.000000      0.000000\n50%    2.133658e+09        0.000000        0.000000      0.000000\n75%    3.211645e+09        1.000000        1.000000      1.000000\nmax    4.294947e+09        1.000000        1.000000      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.747700e+04</td>\n      <td>57477.000000</td>\n      <td>57477.000000</td>\n      <td>57477.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.142564e+09</td>\n      <td>0.349079</td>\n      <td>0.341911</td>\n      <td>0.309011</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.238327e+09</td>\n      <td>0.476683</td>\n      <td>0.474354</td>\n      <td>0.462090</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.019200e+04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.071821e+09</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.133658e+09</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.211645e+09</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.294947e+09</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['model_a'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.855571Z","iopub.execute_input":"2024-05-17T18:44:41.855971Z","iopub.status.idle":"2024-05-17T18:44:41.877615Z","shell.execute_reply.started":"2024-05-17T18:44:41.855936Z","shell.execute_reply":"2024-05-17T18:44:41.875694Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"model_a\ngpt-4-1106-preview          3678\ngpt-3.5-turbo-0613          3553\ngpt-4-0613                  3099\nclaude-2.1                  2859\ngpt-4-0314                  2087\n                            ... \nfalcon-180b-chat             145\nopenchat-3.5-0106            108\nqwen1.5-7b-chat              106\nqwen1.5-4b-chat              100\nmistral-7b-instruct-v0.2      54\nName: count, Length: 64, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['model_b'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.879472Z","iopub.execute_input":"2024-05-17T18:44:41.879894Z","iopub.status.idle":"2024-05-17T18:44:41.901191Z","shell.execute_reply.started":"2024-05-17T18:44:41.879860Z","shell.execute_reply":"2024-05-17T18:44:41.899496Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"model_b\ngpt-4-1106-preview          3709\ngpt-3.5-turbo-0613          3530\ngpt-4-0613                  3066\nclaude-2.1                  2724\nclaude-instant-1            2051\n                            ... \nfalcon-180b-chat             141\nopenchat-3.5-0106            136\nqwen1.5-7b-chat              102\nqwen1.5-4b-chat              100\nmistral-7b-instruct-v0.2      46\nName: count, Length: 64, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The top 4 models that won most often as model a or as model b are:\n- gpt-4-1106-preview\n- gpt-4-0613\n- gpt-3.5-turbo-0613\n- gpt-4-0314","metadata":{}},{"cell_type":"code","source":"# Creating a function to perform cleaning steps at once (Removes numbers and unnecessary characters, makes all letters lowercase, removes stopwords)\nnltk.download('stopwords')\nstopwords_list = stopwords.words('english')\n\nno_bad_chars = re.compile('[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n - ]')\n# no_nums = re.compile('[\\d-]')\n\ndef clean_text(text):\n#     text = no_nums.sub('', text)\n    text = no_bad_chars.sub(' ', text)\n    text = text.lower()\n    text = ' '.join(word for word in text.split() if word not in stopwords_list)\n    return text\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.903408Z","iopub.execute_input":"2024-05-17T18:44:41.903851Z","iopub.status.idle":"2024-05-17T18:44:41.917115Z","shell.execute_reply.started":"2024-05-17T18:44:41.903815Z","shell.execute_reply":"2024-05-17T18:44:41.915463Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"train_cleaned = train.copy()\ntrain['model_a'] = train['model_a'].astype(str)\ntrain['model_b'] = train['model_b'].astype(str)\ntrain['prompt'] = train['prompt'].astype(str)\ntrain['response_a'] = train['response_a'].astype(str)\ntrain['response_b'] = train['response_b'].astype(str)\n\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:41.921711Z","iopub.execute_input":"2024-05-17T18:44:41.922453Z","iopub.status.idle":"2024-05-17T18:44:41.957400Z","shell.execute_reply.started":"2024-05-17T18:44:41.922396Z","shell.execute_reply":"2024-05-17T18:44:41.955713Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Applying text cleaning function to text columns\n\ntrain_cleaned['model_a'] = (train['model_a']).apply(clean_text)\ntrain_cleaned['model_b'] = (train['model_b']).apply(clean_text)\ntrain_cleaned['prompt'] = (train['prompt']).apply(clean_text)\ntrain_cleaned['response_a'] = (train['response_a']).apply(clean_text)\ntrain_cleaned['response_b'] = (train['response_b']).apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:44:42.134722Z","iopub.execute_input":"2024-05-17T18:44:42.135200Z","iopub.status.idle":"2024-05-17T18:46:00.778894Z","shell.execute_reply.started":"2024-05-17T18:44:42.135162Z","shell.execute_reply":"2024-05-17T18:46:00.777649Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n\n# vectorizer = CountVectorizer()\n# X = vectorizer.fit_transform(train_cleaned[train_cleaned['prompt']])","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.781431Z","iopub.execute_input":"2024-05-17T18:46:00.781925Z","iopub.status.idle":"2024-05-17T18:46:00.788251Z","shell.execute_reply.started":"2024-05-17T18:46:00.781880Z","shell.execute_reply":"2024-05-17T18:46:00.786786Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# from nltk.tokenize import  word_tokenize\n# # tokenization of sentence into words\n# # modela_tokens = word_tokenize(train_cleaned['model_a'])\n# # modelb_tokens = word_tokenize(train_cleaned['model_b'])\n# prompt_tokens = word_tokenize(train_cleaned[train_cleaned['prompt']])\n# responsea_tokens = word_tokenize(train_cleaned[train_cleaned['response_a']])\n# responseb_tokens = word_tokenize(train_cleaned[train_cleaned['response_a']])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.790271Z","iopub.execute_input":"2024-05-17T18:46:00.790734Z","iopub.status.idle":"2024-05-17T18:46:00.803054Z","shell.execute_reply.started":"2024-05-17T18:46:00.790700Z","shell.execute_reply":"2024-05-17T18:46:00.801455Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"\n# from sklearn.linear_model import LogisticRegressionCV\n\n# cv = CountVectorizer(max_df =.98,min_df =.05,max_features=9000,ngram_range=(1,3),stop_words = stopwords_list)\n\n# lr  = LogisticRegressionCV(\n# penalty='l1', \n# class_weight=\"balanced\",  #accounts for class imbalance\n# max_iter = 5000,\n# solver =\"saga\",             #faster convergence\n# multi_class =\"multinomial\", #accounts for multiclass\n# cv=5, #accounts for picking the best score out of all the splits\n# scoring='neg_log_loss'       #f1 for imbalanced classification\n# )\n# X = train_cleaned.loc[:,\"response_b\"]\n# y= train_cleaned[['winner_model_a', 'winner_model_b', 'winner_tie']]\n# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)\n# pipe = Pipeline([(\"c_v\",cv),(\"lr\",MultiOutputClassifier(lr))])\n# pipe.fit(X_train,y_train)\n# # predictions = pipe.predict_proba(X_test )\n\n# probability_class_1 = pipe.predict_proba(X_test)[:, 1]\n# probability_class_2 = pipe.predict_proba(X_test)[:, 2]\n# probability_class_3 = pipe.predict_proba(X_test)[:, 3]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.806659Z","iopub.execute_input":"2024-05-17T18:46:00.807116Z","iopub.status.idle":"2024-05-17T18:46:00.816667Z","shell.execute_reply.started":"2024-05-17T18:46:00.807079Z","shell.execute_reply":"2024-05-17T18:46:00.815221Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# pipe.classes_\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.818654Z","iopub.execute_input":"2024-05-17T18:46:00.819116Z","iopub.status.idle":"2024-05-17T18:46:00.834831Z","shell.execute_reply.started":"2024-05-17T18:46:00.819080Z","shell.execute_reply":"2024-05-17T18:46:00.833084Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# predictions ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.836511Z","iopub.execute_input":"2024-05-17T18:46:00.836984Z","iopub.status.idle":"2024-05-17T18:46:00.850428Z","shell.execute_reply.started":"2024-05-17T18:46:00.836941Z","shell.execute_reply":"2024-05-17T18:46:00.848459Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# print(log_loss(predictions, y_test))\n# print(classification_report(predictions, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.853234Z","iopub.execute_input":"2024-05-17T18:46:00.853765Z","iopub.status.idle":"2024-05-17T18:46:00.863800Z","shell.execute_reply.started":"2024-05-17T18:46:00.853726Z","shell.execute_reply":"2024-05-17T18:46:00.862271Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# rf = RandomForestClassifier(\n# # penalty='l1', \n# class_weight=\"balanced\",  #accounts for class imbalance\n# # max_iter = 5000, #fixes convergence error\n# # solver =\"saga\",             #faster convergence\n# # multi_class =\"multinomial\", #accounts for multiclass\n# # cv=5, \n# # scoring='neg_log_loss'       \n# )\n\n# pipe2 = Pipeline([(\"c_v\",cv),(\"rf\",MultiOutputClassifier(rf))])\n# pipe2.fit(X_train,y_train)\n# preds2 = pipe2.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.865641Z","iopub.execute_input":"2024-05-17T18:46:00.866166Z","iopub.status.idle":"2024-05-17T18:46:00.877944Z","shell.execute_reply.started":"2024-05-17T18:46:00.866125Z","shell.execute_reply":"2024-05-17T18:46:00.876479Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# preds2","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.879761Z","iopub.execute_input":"2024-05-17T18:46:00.881022Z","iopub.status.idle":"2024-05-17T18:46:00.889807Z","shell.execute_reply.started":"2024-05-17T18:46:00.880973Z","shell.execute_reply":"2024-05-17T18:46:00.888149Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# print(log_loss(preds2, y_test))\n# print(classification_report(preds2, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.894741Z","iopub.execute_input":"2024-05-17T18:46:00.895142Z","iopub.status.idle":"2024-05-17T18:46:00.903803Z","shell.execute_reply.started":"2024-05-17T18:46:00.895111Z","shell.execute_reply":"2024-05-17T18:46:00.902396Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"____","metadata":{}},{"cell_type":"code","source":"# target = train_cleaned[['winner_model_a', 'winner_model_b', 'winner_tie']]\ntrain_cleaned['combined_text'] = train_cleaned['prompt'] + ' ' + train_cleaned['response_a'] + ' ' + train_cleaned['response_b']\ndata = train_cleaned['combined_text'].map(word_tokenize).values\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:46:00.905431Z","iopub.execute_input":"2024-05-17T18:46:00.905880Z","iopub.status.idle":"2024-05-17T18:49:50.839845Z","shell.execute_reply.started":"2024-05-17T18:46:00.905834Z","shell.execute_reply":"2024-05-17T18:49:50.838099Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# data","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:49:58.556010Z","iopub.execute_input":"2024-05-17T18:49:58.556511Z","iopub.status.idle":"2024-05-17T18:49:58.562551Z","shell.execute_reply.started":"2024-05-17T18:49:58.556473Z","shell.execute_reply":"2024-05-17T18:49:58.561043Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"total_vocabulary = set(word for prompt in data for word in prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:49:59.599245Z","iopub.execute_input":"2024-05-17T18:49:59.599721Z","iopub.status.idle":"2024-05-17T18:50:03.511541Z","shell.execute_reply.started":"2024-05-17T18:49:59.599686Z","shell.execute_reply":"2024-05-17T18:50:03.509884Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"len(total_vocabulary)\nprint('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:03.514046Z","iopub.execute_input":"2024-05-17T18:50:03.514597Z","iopub.status.idle":"2024-05-17T18:50:03.522115Z","shell.execute_reply.started":"2024-05-17T18:50:03.514551Z","shell.execute_reply":"2024-05-17T18:50:03.520705Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"There are 445532 unique tokens in the dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# glove = {}\n# with open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt', 'rb') as f:\n#     for line in f:\n#         parts = line.split()\n#         word = parts[0].decode('utf-8')\n#         if word in total_vocabulary:\n#             vector = np.array(parts[1:], dtype=np.float32)\n#             glove[word] = vector","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:06.924999Z","iopub.execute_input":"2024-05-17T18:50:06.925446Z","iopub.status.idle":"2024-05-17T18:50:06.933433Z","shell.execute_reply.started":"2024-05-17T18:50:06.925412Z","shell.execute_reply":"2024-05-17T18:50:06.931636Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# #random word to see if it worked\n# glove['write']","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:07.141274Z","iopub.execute_input":"2024-05-17T18:50:07.141756Z","iopub.status.idle":"2024-05-17T18:50:07.147996Z","shell.execute_reply.started":"2024-05-17T18:50:07.141721Z","shell.execute_reply":"2024-05-17T18:50:07.146257Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# #creating mean word embeddings\n# class W2vVectorizer(object):\n    \n#     def __init__(self, w2v):\n#         # Takes in a dictionary of words and vectors as input\n#         self.w2v = w2v\n#         if len(w2v) == 0:\n#             self.dimensions = 0\n#         else:\n#             self.dimensions = len(w2v[next(iter(glove))])\n    \n#     # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n#     # it can't be used in a scikit-learn pipeline  \n#     def fit(self, X, y):\n#         return self\n            \n#     def transform(self, X):\n#         return np.array([\n#             np.mean([self.w2v[w] for w in words if w in self.w2v]\n#                    or [np.zeros(self.dimensions)], axis=0) for words in X])","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:07.446530Z","iopub.execute_input":"2024-05-17T18:50:07.446927Z","iopub.status.idle":"2024-05-17T18:50:07.453844Z","shell.execute_reply.started":"2024-05-17T18:50:07.446897Z","shell.execute_reply":"2024-05-17T18:50:07.452107Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import cross_val_score\n\n# rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n#               ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n#                 ('Support Vector Machine', SVC())])\n# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n#               ('Logistic Regression', LogisticRegression())])","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:07.644401Z","iopub.execute_input":"2024-05-17T18:50:07.644807Z","iopub.status.idle":"2024-05-17T18:50:07.651158Z","shell.execute_reply.started":"2024-05-17T18:50:07.644777Z","shell.execute_reply":"2024-05-17T18:50:07.649693Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# models = [('Random Forest', rf),\n#           ('Support Vector Machine', svc),\n#           ('Logistic Regression', lr)]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:07.899457Z","iopub.execute_input":"2024-05-17T18:50:07.899880Z","iopub.status.idle":"2024-05-17T18:50:07.905372Z","shell.execute_reply.started":"2024-05-17T18:50:07.899847Z","shell.execute_reply":"2024-05-17T18:50:07.904009Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def target(data):\n    y1 = data.pop('winner_model_a')\n    y1 = np.array(y1)\n    y2 = data.pop('winner_model_b')\n    y2 = np.array(y2)\n    y3 = data.pop('winner_tie')\n    y3 = np.array(y3)\n    return y1, y2, y3\n\ntarget = target(train_cleaned)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:08.392989Z","iopub.execute_input":"2024-05-17T18:50:08.393665Z","iopub.status.idle":"2024-05-17T18:50:08.405208Z","shell.execute_reply.started":"2024-05-17T18:50:08.393626Z","shell.execute_reply":"2024-05-17T18:50:08.403699Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"target_t = np.transpose(target)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:09.859446Z","iopub.execute_input":"2024-05-17T18:50:09.859945Z","iopub.status.idle":"2024-05-17T18:50:09.867887Z","shell.execute_reply.started":"2024-05-17T18:50:09.859908Z","shell.execute_reply":"2024-05-17T18:50:09.866469Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# # ‚è∞ This cell may take several minutes to run\n# scores = [(name, cross_val_score(model, data, target_t, cv=2).mean()) for name, model, in models]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:19.032042Z","iopub.execute_input":"2024-05-17T18:50:19.032584Z","iopub.status.idle":"2024-05-17T18:50:19.038889Z","shell.execute_reply.started":"2024-05-17T18:50:19.032543Z","shell.execute_reply":"2024-05-17T18:50:19.037138Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# scores","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:21.956628Z","iopub.execute_input":"2024-05-17T18:50:21.957085Z","iopub.status.idle":"2024-05-17T18:50:21.963453Z","shell.execute_reply.started":"2024-05-17T18:50:21.957049Z","shell.execute_reply":"2024-05-17T18:50:21.961454Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, LSTM, Embedding\nfrom keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:22.714846Z","iopub.execute_input":"2024-05-17T18:50:22.715398Z","iopub.status.idle":"2024-05-17T18:50:22.723309Z","shell.execute_reply.started":"2024-05-17T18:50:22.715357Z","shell.execute_reply":"2024-05-17T18:50:22.721622Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(train_cleaned) * .8)\nlabels = target_t\n\n# prompt_train = train_cleaned['prompt'][:train_size]\n# response_a_train = train_cleaned['response_a'][:train_size]\n# response_b_train = train_cleaned['response_b'][:train_size]\ntext_train =  train_cleaned['combined_text'][:train_size]\n\nlabels_train = labels[:train_size]\n\n\n# prompt_test = train_cleaned['prompt'][train_size:]\n# response_a_test = train_cleaned['response_a'][train_size:]\n# response_b_test = train_cleaned['response_b'][train_size:]\n\nlabels_test = labels[train_size:]\n\ntext_test =  train_cleaned['combined_text'][train_size:]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:24.182816Z","iopub.execute_input":"2024-05-17T18:50:24.183405Z","iopub.status.idle":"2024-05-17T18:50:24.192265Z","shell.execute_reply.started":"2024-05-17T18:50:24.183353Z","shell.execute_reply":"2024-05-17T18:50:24.190602Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#deep learning with word embeddings\n#tokenize\ntokenizer = text.Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(text_train))\nlist_tokenized_prmpt_rsps = tokenizer.texts_to_sequences(text_train)\nX_t = sequence.pad_sequences(list_tokenized_prmpt_rsps, maxlen=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:50:28.838969Z","iopub.execute_input":"2024-05-17T18:50:28.839446Z","iopub.status.idle":"2024-05-17T18:50:58.573818Z","shell.execute_reply.started":"2024-05-17T18:50:28.839410Z","shell.execute_reply":"2024-05-17T18:50:58.572222Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"list_tokenized_prmpt_rsps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model()\n# inputs = [x], outputs=[output1, output2, output3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_size = 128\nx = layers.Embedding(20000, embedding_size)\n# x = layers.LSTM(25, return_sequences=True)(x)\n# x = layers.GlobalMaxPool1D(name='x', built=False)(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\nsecond_dense = layers.Dense(128, activation='relu')(x)\noutput1 = layers.Dense(1, activation='softmax')(second_dense)\nthird_dense =layers.Dense(64, activation='relu')(output1)\noutput2 = layers.Dense(1, activation='softmax')(third_dense)\nfourth_dense = layers.Dense(32, activation='relu')(output2)\noutput3 = layers.Dense(1, activation='softmax')(fourth_dense)\n\n\n\n\n# model.add(layers.Embedding(20000, embedding_size))\n# model.add(layers.LSTM(25, return_sequences=True))\n# model.add(layers.GlobalMaxPool1D())\n# model.add(layers.Dropout(0.5))\n# model.add(layers.Dense(50, activation='relu'))\n# model.add(layers.Dropout(0.5))\n# model.add(layers.Dense(1, activation='softmax'))\n# model.add(layers.Dense(1, activation='softmax'))\n# model.add(layers.Dense(1, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape=(len(np.transpose(X_t)),))\nembedding_size = 128\nembedding = layers.Embedding(20000, embedding_size)\n\nfirst_dense = Dense(units=128, activation='relu')(input_layer)\nsecond_dense = Dense(units=128, activation='softmax')(first_dense)\n\ny1_output = Dense(units=1, name='y1_output')(second_dense)\nthird_dense = Dense(units=64, activation='softmax')(second_dense)\n\ny2_output = Dense(units=1, name='y2_output')(third_dense)\nfourth_dense = Dense(units=64, activation='softmax')(third_dense)\n\ny3_output = Dense(units=1, name='y3_output')(fourth_dense)\n\nmodel = Model(inputs=input_layer, outputs=[y1_output, y2_output, y3_output])\n\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', \n              loss={'y1_output': 'BinaryCrossentropy', 'y2_output': 'BinaryCrossentropy', 'y3_output':'BinaryCrossentropy'},\n              metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n                       'y2_output': tf.keras.metrics.RootMeanSquaredError(),\n                       'y3_output': tf.keras.metrics.RootMeanSquaredError()})\n\n# loss={'y1_output': 'mse', 'y2_output': 'mse'},\n#               metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n#                        'y2_output': tf.keras.metrics.RootMeanSquaredError()})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_t.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_layer.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_t, target_t, epochs=1, batch_size=32, validation_split=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_model.fit([[prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3]], labels_train, epochs=10, batch_size=128)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # input_layer = Input(shape=(len(train_set.columns),))\n# first_dense = Dense(units=128, activation='softmax')(merged_layer)\n# second_dense = Dense(units=128, activation='softmax')(first_dense)\n\n# y1_output = Dense(units=1, name='y1_output')(second_dense)\n\n# third_dense = Dense(units=64, activation='softmax')(second_dense)\n# y2_output = Dense(units=1, name='y2_output')(third_dense)\n\n# fourth_dense = Dense(units=32, activation='softmax')(third_dense)\n# y3_output = Dense(units=1, name='y3_output')(fourth_dense)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def target(data):\n    y1 = data.pop('winner_model_a')\n    y1 = np.array(y1)\n    y2 = data.pop('winner_model_b')\n    y2 = np.array(y2)\n    y3 = data.pop('winner_tie')\n    y3 = np.array(y3)\n    return y1, y2, y3\n\ntarget = target(train_cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_t = np.transpose(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(train_cleaned) * .8)\nlabels = target_t\n\nprompt_train = train_cleaned['prompt'][:train_size]\nresponse_a_train = train_cleaned['response_a'][:train_size]\nresponse_b_train = train_cleaned['response_b'][:train_size]\n\nlabels_train = labels[:train_size]\n\n\nprompt_test = train_cleaned['prompt'][train_size:]\nresponse_a_test = train_cleaned['response_a'][train_size:]\nresponse_b_test = train_cleaned['response_b'][train_size:]\n\nlabels_test = labels[train_size:]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 12000\ntokenize = keras.preprocessing.text.Tokenizer(num_words = vocab_size, char_level=False)\ntokenize.fit_on_texts(prompt_train)\ntokenize.fit_on_texts(response_a_train)\ntokenize.fit_on_texts(response_b_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_bow_train = tokenize.texts_to_matrix(prompt_train)\nprompt_bow_test = tokenize.texts_to_matrix(prompt_test)\nresponsea_bow_train = tokenize.texts_to_matrix(response_a_train)\nresponsea_bow_test = tokenize.texts_to_matrix(response_a_test)\nresponseb_bow_train = tokenize.texts_to_matrix(response_b_train)\nresponseb_bow_test = tokenize.texts_to_matrix(response_b_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers = keras.layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define our wide model with the functional API\nbow_input1 = layers.Input(shape=(vocab_size,))\nbow_input2 = layers.Input(shape=(vocab_size,))\nbow_input3 = layers.Input(shape=(vocab_size,))\nmerged_layer = layers.concatenate([bow_input1, bow_input2, bow_input3])\nmerged_layer = layers.Dense(256, activation='softmax')(merged_layer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_layer = Input(shape=(len(train_set.columns),))\nfirst_dense = Dense(units=128, activation='softmax')(merged_layer)\nsecond_dense = Dense(units=128, activation='softmax')(first_dense)\n\ny1_output = Dense(units=1, name='y1_output')(second_dense)\n\nthird_dense = Dense(units=64, activation='softmax')(second_dense)\ny2_output = Dense(units=1, name='y2_output')(third_dense)\n\nfourth_dense = Dense(units=32, activation='softmax')(third_dense)\ny3_output = Dense(units=1, name='y3_output')(fourth_dense)\n# sixth_dense = Dense(units='64', activation='relu')(fifth_dense)\n\n# model = Model(inputs=input_layer, outputs=[y1_output, y2_output, y3_output])\n\n# print(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = layers.Dense(1)(merged_layer)\nwide_model = keras.Model(inputs=[bow_input1, bow_input2, bow_input3], outputs=[y1_output, y2_output, y3_output])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wide_model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])\nprint(wide_model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Deep Model","metadata":{}},{"cell_type":"code","source":"train_embed1 = tokenize.texts_to_sequences(prompt_train)\ntest_embed1 = tokenize.texts_to_sequences(prompt_test)\n\ntrain_embed2 = tokenize.texts_to_sequences(response_a_train)\ntest_embed2 = tokenize.texts_to_sequences(response_a_test)\n\ntrain_embed3 = tokenize.texts_to_sequences(response_b_train)\ntest_embed3 = tokenize.texts_to_sequences(response_b_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 170\ntrain_embed1 = keras.preprocessing.sequence.pad_sequences(train_embed1, maxlen=max_seq_length)\ntest_embed1 = keras.preprocessing.sequence.pad_sequences(test_embed1, maxlen=max_seq_length)\n\ntrain_embed2 = keras.preprocessing.sequence.pad_sequences(train_embed2, maxlen=max_seq_length)\ntest_embed2 = keras.preprocessing.sequence.pad_sequences(test_embed2, maxlen=max_seq_length)\n\ntrain_embed3 = keras.preprocessing.sequence.pad_sequences(train_embed3, maxlen=max_seq_length)\ntest_embed3 = keras.preprocessing.sequence.pad_sequences(test_embed3, maxlen=max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deep_inputs = layers.Input(shape=(max_seq_length,))\nembedding1 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\nembedding1 = layers.Flatten()(embedding1)\nembedding2 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\nembedding2 = layers.Flatten()(embedding2)\nembedding3 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\nembedding3 = layers.Flatten()(embedding3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_out1 = layers.Dense(1, name='embed_out1')(embedding1)\nembed_out2 = layers.Dense(1, name='embed_out2')(embedding2)\nembed_out3 = layers.Dense(1, name='embed_out3')(embedding3)\n# y2_output = Dense(units=1, name='y2_output')(third_dense)\ndeep_model = Model(inputs=[embedding1, embedding2, embedding3], outputs=[embed_out1, embed_out2, embed_out3])\ndeep_model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(deep_model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wide_model.input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[deep_model.input]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine wide and deep into one model\n# merged_out = layers.concatenate([wide_model.output, deep_model.output])\n# merged_out = layers.Dense(1)(merged_out)\ncombined_model = keras.Model(wide_model.input + [deep_model.input], [wide_model.output, deep_model.output])\nprint(combined_model.summary())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_model.compile(loss='BinaryCrossentropy',\n                       optimizer='adam',\n                       metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_inputs = [prompt_bow_train, responsea_bow_train, responseb_bow_train] + [train_embed1, train_embed2, train_embed3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embed1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run training\n# combined_model.fit([[prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3]], labels_train, epochs=10, batch_size=128)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_model.evaluate([prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3], labels_test, batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generate predictions\n# predictions = combined_model.predict([description_bow_test, variety_test] + [test_embed])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}