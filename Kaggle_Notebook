{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Competition Notebook:\"LMSYS - Chatbot Arena Human Preference Predictions\"","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I am conducting data exploration, analysis, and modeling in order to generate predictions for which Chatbot model will be preferred for a given prompt.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-07T15:42:09.501459Z","iopub.execute_input":"2024-07-07T15:42:09.501875Z","iopub.status.idle":"2024-07-07T15:42:10.906024Z","shell.execute_reply.started":"2024-07-07T15:42:09.501840Z","shell.execute_reply":"2024-07-07T15:42:10.904640Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\n/kaggle/input/lmsys-chatbot-arena/train.csv\n/kaggle/input/lmsys-chatbot-arena/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport seaborn as sns\nsns.set(font_scale=2)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.tokenize import RegexpTokenizer, word_tokenize\nfrom sklearn.metrics import log_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom nltk import FreqDist\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\n\nfrom gensim.models import word2vec","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:42:10.908383Z","iopub.execute_input":"2024-07-07T15:42:10.908990Z","iopub.status.idle":"2024-07-07T15:42:32.096784Z","shell.execute_reply.started":"2024-07-07T15:42:10.908947Z","shell.execute_reply":"2024-07-07T15:42:32.095662Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras==2.15.0","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:42:32.102966Z","iopub.execute_input":"2024-07-07T15:42:32.103303Z","iopub.status.idle":"2024-07-07T15:42:51.671317Z","shell.execute_reply.started":"2024-07-07T15:42:32.103274Z","shell.execute_reply":"2024-07-07T15:42:51.669858Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting keras==2.15.0\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.2.1\n    Uninstalling keras-3.2.1:\n      Successfully uninstalled keras-3.2.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tensorflow import keras\nimport keras\n# import keras_nlp\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:42:51.673292Z","iopub.execute_input":"2024-07-07T15:42:51.673851Z","iopub.status.idle":"2024-07-07T15:43:05.899506Z","shell.execute_reply.started":"2024-07-07T15:42:51.673794Z","shell.execute_reply":"2024-07-07T15:43:05.898248Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-07-07 15:42:53.843652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-07 15:42:53.843829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-07 15:42:54.008022: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\n\nfrom keras.layers import Dense, Input\n\nfrom tqdm.auto import tqdm\n\nfrom keras.layers import TextVectorization\nfrom keras import Sequential\nfrom keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:05.900941Z","iopub.execute_input":"2024-07-07T15:43:05.901688Z","iopub.status.idle":"2024-07-07T15:43:05.916260Z","shell.execute_reply.started":"2024-07-07T15:43:05.901650Z","shell.execute_reply":"2024-07-07T15:43:05.914198Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Loading the train and test datasets:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:08.547898Z","iopub.execute_input":"2024-07-07T15:43:08.548349Z","iopub.status.idle":"2024-07-07T15:43:12.428999Z","shell.execute_reply.started":"2024-07-07T15:43:08.548313Z","shell.execute_reply":"2024-07-07T15:43:12.427826Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.431335Z","iopub.execute_input":"2024-07-07T15:43:12.431728Z","iopub.status.idle":"2024-07-07T15:43:12.462118Z","shell.execute_reply.started":"2024-07-07T15:43:12.431695Z","shell.execute_reply":"2024-07-07T15:43:12.460941Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.463808Z","iopub.execute_input":"2024-07-07T15:43:12.464271Z","iopub.status.idle":"2024-07-07T15:43:12.479616Z","shell.execute_reply.started":"2024-07-07T15:43:12.464232Z","shell.execute_reply":"2024-07-07T15:43:12.478232Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>[\"I have three oranges today, I ate an orange ...</td>\n      <td>[\"You have two oranges today.\"]</td>\n      <td>[\"You still have three oranges. Eating an oran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>[\"You are a mediator in a heated political deb...</td>\n      <td>[\"Thank you for sharing the details of the sit...</td>\n      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>[\"How to initialize the classification head wh...</td>\n      <td>[\"When you want to initialize the classificati...</td>\n      <td>[\"To initialize the classification head when p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.482745Z","iopub.execute_input":"2024-07-07T15:43:12.483083Z","iopub.status.idle":"2024-07-07T15:43:12.554457Z","shell.execute_reply.started":"2024-07-07T15:43:12.483057Z","shell.execute_reply":"2024-07-07T15:43:12.553101Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.556068Z","iopub.execute_input":"2024-07-07T15:43:12.556521Z","iopub.status.idle":"2024-07-07T15:43:12.592815Z","shell.execute_reply.started":"2024-07-07T15:43:12.556489Z","shell.execute_reply":"2024-07-07T15:43:12.591483Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                 id  winner_model_a  winner_model_b    winner_tie\ncount  5.747700e+04    57477.000000    57477.000000  57477.000000\nmean   2.142564e+09        0.349079        0.341911      0.309011\nstd    1.238327e+09        0.476683        0.474354      0.462090\nmin    3.019200e+04        0.000000        0.000000      0.000000\n25%    1.071821e+09        0.000000        0.000000      0.000000\n50%    2.133658e+09        0.000000        0.000000      0.000000\n75%    3.211645e+09        1.000000        1.000000      1.000000\nmax    4.294947e+09        1.000000        1.000000      1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.747700e+04</td>\n      <td>57477.000000</td>\n      <td>57477.000000</td>\n      <td>57477.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.142564e+09</td>\n      <td>0.349079</td>\n      <td>0.341911</td>\n      <td>0.309011</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.238327e+09</td>\n      <td>0.476683</td>\n      <td>0.474354</td>\n      <td>0.462090</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.019200e+04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.071821e+09</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.133658e+09</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.211645e+09</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.294947e+09</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"winner_a = train[train['winner_model_a']==True]","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.594085Z","iopub.execute_input":"2024-07-07T15:43:12.594547Z","iopub.status.idle":"2024-07-07T15:43:12.607322Z","shell.execute_reply.started":"2024-07-07T15:43:12.594508Z","shell.execute_reply":"2024-07-07T15:43:12.605786Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"winner_b = train[train['winner_model_b']==True]","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.609295Z","iopub.execute_input":"2024-07-07T15:43:12.609645Z","iopub.status.idle":"2024-07-07T15:43:12.622160Z","shell.execute_reply.started":"2024-07-07T15:43:12.609617Z","shell.execute_reply":"2024-07-07T15:43:12.620731Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"winner_a['model_a'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.623762Z","iopub.execute_input":"2024-07-07T15:43:12.624209Z","iopub.status.idle":"2024-07-07T15:43:12.638231Z","shell.execute_reply.started":"2024-07-07T15:43:12.624176Z","shell.execute_reply":"2024-07-07T15:43:12.636873Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"model_a\ngpt-4-1106-preview          2019\ngpt-4-0613                  1280\ngpt-3.5-turbo-0613          1213\ngpt-4-0314                  1033\nclaude-2.1                   896\n                            ... \nchatglm2-6b                   35\nqwen1.5-7b-chat               29\nopenchat-3.5-0106             28\nqwen1.5-4b-chat               19\nmistral-7b-instruct-v0.2      15\nName: count, Length: 64, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"winner_b['model_b'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.639747Z","iopub.execute_input":"2024-07-07T15:43:12.640098Z","iopub.status.idle":"2024-07-07T15:43:12.658610Z","shell.execute_reply.started":"2024-07-07T15:43:12.640068Z","shell.execute_reply":"2024-07-07T15:43:12.657471Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"model_b\ngpt-4-1106-preview          2054\ngpt-4-0613                  1170\ngpt-3.5-turbo-0613          1168\ngpt-4-0314                   960\nclaude-1                     880\n                            ... \nchatglm2-6b                   38\nopenchat-3.5-0106             35\nqwen1.5-7b-chat               22\nqwen1.5-4b-chat               16\nmistral-7b-instruct-v0.2      12\nName: count, Length: 64, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The top 4 models that won most often as model a or as model b are:\n- gpt-4-1106-preview\n- gpt-4-0613\n- gpt-3.5-turbo-0613\n- gpt-4-0314","metadata":{}},{"cell_type":"code","source":"# Creating a function to perform cleaning steps at once (Removes numbers and unnecessary characters, makes all letters lowercase, removes stopwords)\nnltk.download('stopwords')\nstopwords_list = stopwords.words('english')\n\nno_bad_chars = re.compile('[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n - ]')\n# no_nums = re.compile('[\\d-]')\n\ndef clean_text(text):\n#     text = no_nums.sub('', text)\n    text = no_bad_chars.sub(' ', text)\n    text = text.lower()\n    text = ' '.join(word for word in text.split() if word not in stopwords_list)\n    return text\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:12.662651Z","iopub.execute_input":"2024-07-07T15:43:12.663153Z","iopub.status.idle":"2024-07-07T15:43:12.684446Z","shell.execute_reply.started":"2024-07-07T15:43:12.663092Z","shell.execute_reply":"2024-07-07T15:43:12.683110Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"train_cleaned = train.copy()\ntrain['model_a'] = train['model_a'].astype(str)\ntrain['model_b'] = train['model_b'].astype(str)\ntrain['prompt'] = train['prompt'].astype(str)\ntrain['response_a'] = train['response_a'].astype(str)\ntrain['response_b'] = train['response_b'].astype(str)\n\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:58.228360Z","iopub.execute_input":"2024-07-07T15:43:58.228823Z","iopub.status.idle":"2024-07-07T15:43:58.261888Z","shell.execute_reply.started":"2024-07-07T15:43:58.228787Z","shell.execute_reply":"2024-07-07T15:43:58.260591Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Applying text cleaning function to text columns\n\ntrain_cleaned['model_a'] = (train['model_a']).apply(clean_text)\ntrain_cleaned['model_b'] = (train['model_b']).apply(clean_text)\ntrain_cleaned['prompt'] = (train['prompt']).apply(clean_text)\ntrain_cleaned['response_a'] = (train['response_a']).apply(clean_text)\ntrain_cleaned['response_b'] = (train['response_b']).apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:43:58.358446Z","iopub.execute_input":"2024-07-07T15:43:58.358883Z","iopub.status.idle":"2024-07-07T15:45:17.223582Z","shell.execute_reply.started":"2024-07-07T15:43:58.358848Z","shell.execute_reply":"2024-07-07T15:45:17.222202Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# # possible visualization??\n# import graphviz \n\n# dot = graphviz.Digraph(comment='Neural Network')\n\n# dot.node('x', label='Inputs', color='yellow')\n# dot.node('w', label='Weights')\n# dot.node('b', label='Biases')  \n# dot.node('z', label='Output', shape='rectangle')\n\n# dot.edge('x', 'w')\n# dot.edge('w', 'z')\n# dot.edge('b', 'z')\n\n# dot.render('neural_network')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:47:04.573473Z","iopub.execute_input":"2024-07-07T15:47:04.574380Z","iopub.status.idle":"2024-07-07T15:47:04.580172Z","shell.execute_reply.started":"2024-07-07T15:47:04.574334Z","shell.execute_reply":"2024-07-07T15:47:04.578754Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Preparing the data for modeling with multiple outputs:","metadata":{}},{"cell_type":"code","source":"# target = train_cleaned[['winner_model_a', 'winner_model_b', 'winner_tie']]\ntrain_cleaned['combined_text'] = train_cleaned['prompt'] + ' ' + train_cleaned['response_a'] + ' ' + train_cleaned['response_b']\ndata = train_cleaned['combined_text'].map(word_tokenize).values\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:47:06.835307Z","iopub.execute_input":"2024-07-07T15:47:06.836378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_vocabulary = set(word for prompt in data for word in prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(total_vocabulary)\nprint('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def target(data):\n    y1 = data.pop('winner_model_a')\n    y1 = np.array(y1)\n    y2 = data.pop('winner_model_b')\n    y2 = np.array(y2)\n    y3 = data.pop('winner_tie')\n    y3 = np.array(y3)\n    return y1, y2, y3\n\ntarget = target(train_cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_t = np.transpose(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores = [(cross_val_score(model1, data, target_t, cv=2).mean())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, LSTM, Embedding\nfrom keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(train_cleaned) * .8)\nlabels = target_t\n\ntext_train =  train_cleaned['combined_text'][:train_size]\n\nlabels_train = labels[:train_size]\n\nlabels_test = labels[train_size:]\n\ntext_test =  train_cleaned['combined_text'][train_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deep learning with word embeddings\n#tokenize\ntokenizer = text.Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(text_train))\nlist_tokenized_prmpt_rsps = tokenizer.texts_to_sequences(text_train)\nX_t = sequence.pad_sequences(list_tokenized_prmpt_rsps, maxlen=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(list(text_test))\nlist_tokenized_prmpt_rsps_test = tokenizer.texts_to_sequences(text_test)\nX_t_test = sequence.pad_sequences(list_tokenized_prmpt_rsps_test, maxlen=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Multioutput classifier wrapper on log reg\n# from sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.multiclass import OneVsRestClassifier\nlr = LogisticRegression()\nclassifier1 = ClassifierChain(lr)\n# Creating the MultiOutput Classifier with Logistic Regression as the base estimator\n# classifier1 = MultiOutputClassifier(LogisticRegression())\n\n# Fitting the classifier on the training data\n# classifier1.fit(X_t, labels_train)\n\n# Making predictions on the test set\n# predictions1 = classifier1.predict_proba(X_t_test)\n\nbase_lr = LogisticRegression(random_state=0)\nchain = ClassifierChain(base_lr, order='random', random_state=0)\npreds_c = chain.fit(X_t, labels_train).predict_proba(X_t_test)\n# preds = classifier1.predict(X_t_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.multioutput import ClassifierChain\nfrom sklearn.metrics import jaccard_score\nmax_iter=10\n\nchains = [ClassifierChain(base_lr, order=\"random\", random_state=i) for i in range(10)]\nfor chain in chains:\n    chain.fit(X_t, labels_train)\n\nY_pred_chains = np.array([chain.predict_proba(X_t_test) for chain in chains])\nchain_jaccard_scores = [\n    jaccard_score(labels_test, Y_pred_chain >= 0.5, average=\"samples\")\n    for Y_pred_chain in Y_pred_chains\n]\n\nY_pred_ensemble = Y_pred_chains.mean(axis=0)\nensemble_jaccard_score = jaccard_score(\n    labels_test, Y_pred_ensemble >= 0.5, average=\"samples\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import ClassifierChain\n\nbase_lr = LogisticRegression(solver='lbfgs', random_state=0)\nchain = ClassifierChain(base_lr, order='random', random_state=0, chain_method='predict')\nchain.fit(X_t, labels_train).predict(X_t_test)\nchain.predict_proba(X_t_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probas = classifier1.predict_proba(X_t_test)\nprobas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = classifier1.predict(X_t_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_loss(preds,labels_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(log_loss(preds_c, labels_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions1[0][1].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlabels_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Multioutput classifier wrapper on random forest\n# from sklearn.datasets import make_multilabel_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating the MultiOutput Classifier with Logistic Regression as the base estimator\nclassifier2 = OneVsRestClassifier(RandomForestClassifier())\n\n# Fitting the classifier on the training data\nclassifier2.fit(X_t, labels_train)\n\n# Making predictions on the test set\n# predictions2 = classifier.predict_proba(X_t_test)\n\n\n# print(log_loss(predictions, labels_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probas2 = classifier2.predict_proba(X_t_test)\nprobas2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2 = classifier2.predict(X_t_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(log_loss(preds2, target_t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding_size = 128\n# x = layers.Embedding(20000, embedding_size)\n# # x = layers.LSTM(25, return_sequences=True)(x)\n# # x = layers.GlobalMaxPool1D(name='x', built=False)(x)\n# x = layers.Dropout(0.5)(x)\n# x = layers.Dense(128, activation='relu')(x)\n# x = layers.Dropout(0.5)(x)\n# second_dense = layers.Dense(128, activation='relu')(x)\n# output1 = layers.Dense(1, activation='softmax')(second_dense)\n# third_dense =layers.Dense(64, activation='relu')(output1)\n# output2 = layers.Dense(1, activation='softmax')(third_dense)\n# fourth_dense = layers.Dense(32, activation='relu')(output2)\n# output3 = layers.Dense(1, activation='softmax')(fourth_dense)\n\n\n\n\n# # model.add(layers.Embedding(20000, embedding_size))\n# # model.add(layers.LSTM(25, return_sequences=True))\n# # model.add(layers.GlobalMaxPool1D())\n# # model.add(layers.Dropout(0.5))\n# # model.add(layers.Dense(50, activation='relu'))\n# # model.add(layers.Dropout(0.5))\n# # model.add(layers.Dense(1, activation='softmax'))\n# # model.add(layers.Dense(1, activation='softmax'))\n# # model.add(layers.Dense(1, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Dense(128, activation='relu', input_shape=(len(np.transpose(X_t)),)))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(32, activation='relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense((labels_train.shape[1]), activation='softmax')) # <-- Notice activation in final layer.\n\n# scorer = cross_val_score(model1, data, target_t, cv=2).mean()\n\nmodel1.compile(metrics = 'accuracy',\n               loss='binary_crossentropy', # <-- Notice loss function.\n              optimizer='adam',\n              score=(cross_val_score(model1, data, target_t, cv=2).mean()))\n# scorer = check_scoring(model1, scoring=score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_scorer(clf, X, y_true):\n    class_labels = clf.classes_\n    y_pred_proba = clf.predict_proba(X)\n    error = ...\n    return error\n\ngs = GridSearchCV(estimator=KNeighborsClassifier(),\n                  param_grid=[{'n_neighbors': [6]}],\n                  cv=5,\n                  scoring=my_scorer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cross_val_score(model1, X_t, labels_train, cv=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = cross_validate(estimator=estimator,\n                            X=X,\n                            error_score=error_score,\n)\ncv_results[\"test_score\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model1.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [(cross_val_score(model1, data, target_t, cv=2).mean())]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(X_t, labels_train, epochs=10, batch_size=100, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.evaluate(X_t_test, labels_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_ = model1.predict(X_t_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"softmax_layer = keras.layers.activations.Softmax()\ninput = np.array([1.0, 2.0, 1.0])\nresult = softmax_layer(input)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Softmax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape=(len(np.transpose(X_t)),))\nembedding_size = 128\nembedding = layers.Embedding(20000, embedding_size)\n\nfirst_dense = Dense(units=128, activation='relu')(input_layer)\nsecond_dense = Dense(units=128, activation='relu')(first_dense)\n\ny1_output = Dense(units=1, name='y1_output')(second_dense)\nthird_dense = Dense(units=64, activation='relu')(second_dense)\n\ny2_output = Dense(units=1, name='y2_output')(third_dense)\nfourth_dense = Dense(units=32, activation='softmax')(third_dense)\n\ny3_output = Dense(units=1, name='y3_output')(fourth_dense)\nsm_layer = Softmax()\n# ,  activation='softmax'\n# softmax_layer = Softmax()\n# sm_input = softmax_layer(input_layer)\nmodel = Model(inputs=input_layer, outputs=[y1_output, y2_output, y3_output])\n\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = np.asarray([input_layer])\nlayer = tf.keras.layers.Softmax()\nlayer(inp).numpy()\n# array([[0.21194157, 0.5761169 , 0.21194157]], dtype=float32)\nmask = np.asarray([[True, False, True]], dtype=bool)\nlayer(inp, mask).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', \n              loss={'y1_output': 'BinaryCrossentropy', 'y2_output': 'BinaryCrossentropy', 'y3_output':'BinaryCrossentropy'},\n              metrics={'y1_output': ['Accuracy'],\n                       'y2_output': ['Accuracy'],\n                       'y3_output': ['Accuracy']})\n\n# loss={'y1_output': 'mse', 'y2_output': 'mse'},\n#               metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n#                        'y2_output': tf.keras.metrics.RootMeanSquaredError()})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_t, labels_train, epochs=5, batch_size=100, validation_split=0.2)\n#, batch_size=32 , validation_split=0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_t_test, labels_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generate predictions\npredictions = model.predict(X_t_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.transpose(predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_tuner as kt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_tuner as kt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\n\ndef model_builder(hp):\n    model = Sequential()\n    model.add(Dense(128, activation='relu', input_shape=(len(np.transpose(X_t)),)))\n    model.add(Dropout(0.1))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense((labels_train.shape[1]), activation='softmax'))\n    \n#     model.add(Dense(10, activation='softmax'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\ntuner = kt.Hyperband(\n    model_builder,\n    objective='val_accuracy',\n    max_epochs=10,\n    directory='my_dir',\n    project_name='intro_to_kt'\n)\n\ntuner.search(X_t, labels_train, epochs=50)\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_builder(hp):\n#   model = keras.Sequential()\n#   model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    model = Sequential()\n    model.add(Dense(128, activation='relu', input_shape=(len(np.transpose(X_t)),)))\n#     model.add(Dropout(0.1))\n#     model.add(Dense(64, activation='relu'))\n#     model.add(Dropout(0.1))\n#     model.add(Dense(32, activation='relu'))\n#     model.add(Dropout(0.1))\n#     model.add(Dense((labels_train.shape[1]), activation='softmax'))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(keras.layers.Dense(10))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.BinaryCrossentropy(),\n                metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=10,\n                     factor=3,\n#                      directory='my_dir',\n                     project_name='intro_to_kt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search(X_t, labels_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_grid = dict(epochs=[10,20,30],\n#                   batch_size= [100,200,300] ,\n#                   learning_rate= [0.5, 1],\n                  \n#                  )\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\nparam_grid = dict(learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3], momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9])\n# grid = GridSearchCV(estimator=model1, param_grid=param_grid, n_jobs=-1, cv=3)\n# param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum)\ngrid = GridSearchCV(estimator=model1, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy')\ngrid_result = grid.fit(X_t, labels_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_model.fit([[prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3]], labels_train, epochs=10, batch_size=128)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # input_layer = Input(shape=(len(train_set.columns),))\n# first_dense = Dense(units=128, activation='softmax')(merged_layer)\n# second_dense = Dense(units=128, activation='softmax')(first_dense)\n\n# y1_output = Dense(units=1, name='y1_output')(second_dense)\n\n# third_dense = Dense(units=64, activation='softmax')(second_dense)\n# y2_output = Dense(units=1, name='y2_output')(third_dense)\n\n# fourth_dense = Dense(units=32, activation='softmax')(third_dense)\n# y3_output = Dense(units=1, name='y3_output')(fourth_dense)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vocab_size = 12000\n# tokenize = keras.preprocessing.text.Tokenizer(num_words = vocab_size, char_level=False)\n# tokenize.fit_on_texts(prompt_train)\n# tokenize.fit_on_texts(response_a_train)\n# tokenize.fit_on_texts(response_b_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt_bow_train = tokenize.texts_to_matrix(prompt_train)\n# prompt_bow_test = tokenize.texts_to_matrix(prompt_test)\n# responsea_bow_train = tokenize.texts_to_matrix(response_a_train)\n# responsea_bow_test = tokenize.texts_to_matrix(response_a_test)\n# responseb_bow_train = tokenize.texts_to_matrix(response_b_train)\n# responseb_bow_test = tokenize.texts_to_matrix(response_b_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# layers = keras.layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define our wide model with the functional API\n# bow_input1 = layers.Input(shape=(vocab_size,))\n# bow_input2 = layers.Input(shape=(vocab_size,))\n# bow_input3 = layers.Input(shape=(vocab_size,))\n# merged_layer = layers.concatenate([bow_input1, bow_input2, bow_input3])\n# merged_layer = layers.Dense(256, activation='softmax')(merged_layer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # input_layer = Input(shape=(len(train_set.columns),))\n# first_dense = Dense(units=128, activation='softmax')(merged_layer)\n# second_dense = Dense(units=128, activation='softmax')(first_dense)\n\n# y1_output = Dense(units=1, name='y1_output')(second_dense)\n\n# third_dense = Dense(units=64, activation='softmax')(second_dense)\n# y2_output = Dense(units=1, name='y2_output')(third_dense)\n\n# fourth_dense = Dense(units=32, activation='softmax')(third_dense)\n# y3_output = Dense(units=1, name='y3_output')(fourth_dense)\n# # sixth_dense = Dense(units='64', activation='relu')(fifth_dense)\n\n# # model = Model(inputs=input_layer, outputs=[y1_output, y2_output, y3_output])\n\n# # print(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # predictions = layers.Dense(1)(merged_layer)\n# wide_model = keras.Model(inputs=[bow_input1, bow_input2, bow_input3], outputs=[y1_output, y2_output, y3_output])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wide_model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])\n# print(wide_model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Deep Model","metadata":{}},{"cell_type":"code","source":"# train_embed1 = tokenize.texts_to_sequences(prompt_train)\n# test_embed1 = tokenize.texts_to_sequences(prompt_test)\n\n# train_embed2 = tokenize.texts_to_sequences(response_a_train)\n# test_embed2 = tokenize.texts_to_sequences(response_a_test)\n\n# train_embed3 = tokenize.texts_to_sequences(response_b_train)\n# test_embed3 = tokenize.texts_to_sequences(response_b_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_seq_length = 170\n# train_embed1 = keras.preprocessing.sequence.pad_sequences(train_embed1, maxlen=max_seq_length)\n# test_embed1 = keras.preprocessing.sequence.pad_sequences(test_embed1, maxlen=max_seq_length)\n\n# train_embed2 = keras.preprocessing.sequence.pad_sequences(train_embed2, maxlen=max_seq_length)\n# test_embed2 = keras.preprocessing.sequence.pad_sequences(test_embed2, maxlen=max_seq_length)\n\n# train_embed3 = keras.preprocessing.sequence.pad_sequences(train_embed3, maxlen=max_seq_length)\n# test_embed3 = keras.preprocessing.sequence.pad_sequences(test_embed3, maxlen=max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deep_inputs = layers.Input(shape=(max_seq_length,))\n# embedding1 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\n# embedding1 = layers.Flatten()(embedding1)\n# embedding2 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\n# embedding2 = layers.Flatten()(embedding2)\n# embedding3 = layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\n# embedding3 = layers.Flatten()(embedding3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embed_out1 = layers.Dense(1, name='embed_out1')(embedding1)\n# embed_out2 = layers.Dense(1, name='embed_out2')(embedding2)\n# embed_out3 = layers.Dense(1, name='embed_out3')(embedding3)\n# # y2_output = Dense(units=1, name='y2_output')(third_dense)\n# deep_model = Model(inputs=[embedding1, embedding2, embedding3], outputs=[embed_out1, embed_out2, embed_out3])\n# deep_model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(deep_model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wide_model.input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [deep_model.input]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Combine wide and deep into one model\n# # merged_out = layers.concatenate([wide_model.output, deep_model.output])\n# # merged_out = layers.Dense(1)(merged_out)\n# combined_model = keras.Model(wide_model.input + [deep_model.input], [wide_model.output, deep_model.output])\n# print(combined_model.summary())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_model.compile(loss='BinaryCrossentropy',\n#                        optimizer='adam',\n#                        metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_inputs = [prompt_bow_train, responsea_bow_train, responseb_bow_train] + [train_embed1, train_embed2, train_embed3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_embed1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run training\n# combined_model.fit([[prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3]], labels_train, epochs=10, batch_size=128)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_model.evaluate([prompt_bow_test, responsea_bow_test, responseb_bow_test] + [train_embed1, train_embed2, train_embed3], labels_test, batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generate predictions\n# predictions = combined_model.predict([description_bow_test, variety_test] + [test_embed])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}